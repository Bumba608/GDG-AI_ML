{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Modeling Notebook: Developer Role Classification**","metadata":{}},{"cell_type":"markdown","source":"# **1. Model Training and Evaluation**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay\nimport xgboost as xgb\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\nimport time\n\n# Reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Feature Selection and Data Splitting**\n\n\nWe prepare our dataset by dropping unnecessary or already processed columns. Commit messages are kept separate for the LLM. Afterwards, we split the dataset into train, validation, and test sets, maintaining class distributions with stratified splitting.","metadata":{}},{"cell_type":"code","source":"features_to_drop = [\n    'index',              # ID column\n    'fileextensions',     # Already processed into categories\n    'timeofcommit',       # Temporal features already extracted\n    'commitmessage',      # Will be used separately for LLM\n]\n\nX = processed_df.drop(features_to_drop + ['role'], axis=1)\ny = processed_df['role']\ncommit_messages = processed_df['commitmessage']\n\nprint(\"Final feature matrix shape (traditional models):\", X.shape)\nprint(\"Target shape:\", y.shape)\n\n# Stratified train-validation-test split\nX_train, X_temp, y_train, y_temp, msg_train, msg_temp = train_test_split(\n    X, y, commit_messages,\n    test_size=0.3,\n    random_state=42,\n    stratify=y\n)\n\nX_val, X_test, y_val, y_test, msg_val, msg_test = train_test_split(\n    X_temp, y_temp, msg_temp,\n    test_size=0.5,\n    random_state=42,\n    stratify=y_temp\n)\n\nprint(\"Training samples:\", X_train.shape[0])\nprint(\"Validation samples:\", X_val.shape[0])\nprint(\"Test samples:\", X_test.shape[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preprocessing Pipeline (Traditional Models)**\n\n\n**We preprocess the features using a ColumnTransformer. Numeric features are scaled with RobustScaler to handle skewed distributions, and categorical features are one-hot encoded.**","metadata":{}},{"cell_type":"code","source":"numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = ['committype']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n        ('num', RobustScaler(), numeric_features)\n    ],\n    remainder='passthrough'\n)\n\n# Test preprocessing\nX_train_transformed = preprocessor.fit_transform(X_train)\nprint(\"Transformed training data shape:\", X_train_transformed.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Traditional Machine Learning Models**","metadata":{}},{"cell_type":"markdown","source":"# **Logistic Regression**\n\n\nWe start with a baseline Logistic Regression model. Class imbalance is handled using class_weight='balanced'. Macro F1 is our main metric","metadata":{}},{"cell_type":"code","source":"baseline_model = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(\n        random_state=42,\n        max_iter=1000,\n        class_weight='balanced',\n        multi_class='multinomial'\n    ))\n])\n\nbaseline_model.fit(X_train, y_train)\ny_val_pred_baseline = baseline_model.predict(X_val)\n\nprint(classification_report(y_val, y_val_pred_baseline, digits=3))\nbaseline_metrics = {\n    'macro_f1': f1_score(y_val, y_val_pred_baseline, average='macro'),\n    'accuracy': accuracy_score(y_val, y_val_pred_baseline),\n    'precision': precision_score(y_val, y_val_pred_baseline, average='macro'),\n    'recall': recall_score(y_val, y_val_pred_baseline, average='macro')\n}\n\n# Confusion matrix\ncm = confusion_matrix(y_val, y_val_pred_baseline, labels=baseline_model.classes_)\nConfusionMatrixDisplay(cm, display_labels=baseline_model.classes_).plot(cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Logistic Regression')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Random Forest**\n\n\n**Random Forest is trained next, with class weighting to handle imbalance. This model can capture non-linear patterns better than logistic regression.**","metadata":{}},{"cell_type":"code","source":"random_forest_model = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(\n        random_state=42,\n        n_estimators=100,\n        class_weight='balanced'\n    ))\n])\n\nrandom_forest_model.fit(X_train, y_train)\ny_val_pred_rf = random_forest_model.predict(X_val)\n\nprint(classification_report(y_val, y_val_pred_rf, digits=3))\nrf_metrics = {\n    'macro_f1': f1_score(y_val, y_val_pred_rf, average='macro'),\n    'accuracy': accuracy_score(y_val, y_val_pred_rf),\n    'precision': precision_score(y_val, y_val_pred_rf, average='macro'),\n    'recall': recall_score(y_val, y_val_pred_rf, average='macro')\n}\n\ncm_rf = confusion_matrix(y_val, y_val_pred_rf, labels=random_forest_model.classes_)\nConfusionMatrixDisplay(cm_rf, display_labels=random_forest_model.classes_).plot(cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Random Forest')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **XGBoost**\n\n\n**XGBoost is included as it is powerful for tabular data. We encode the target labels to integers for multi-class classification.**","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.transform(y_val)\n\nxgb_classifier = xgb.XGBClassifier(\n    use_label_encoder=False,\n    eval_metric='merror',\n    random_state=42,\n    objective='multi:softmax',\n    n_estimators=100,\n    learning_rate=0.1\n)\n\nxgb_model = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', xgb_classifier)\n])\n\nxgb_model.fit(X_train, y_train_encoded)\ny_val_pred_xgb_encoded = xgb_model.predict(X_val)\n\nprint(classification_report(y_val_encoded, y_val_pred_xgb_encoded, digits=3, target_names=label_encoder.classes_))\nxgb_metrics = {\n    'macro_f1': f1_score(y_val_encoded, y_val_pred_xgb_encoded, average='macro'),\n    'accuracy': accuracy_score(y_val_encoded, y_val_pred_xgb_encoded),\n    'precision': precision_score(y_val_encoded, y_val_pred_xgb_e**XGBoost**\n\nncoded, average='macro'),\n    'recall': recall_score(y_val_encoded, y_val_pred_xgb_encoded, average='macro')\n}\n\ncm_xgb = confusion_matrix(y_val_encoded, y_val_pred_xgb_encoded)\nConfusionMatrixDisplay(cm_xgb, display_labels=label_encoder.classes_).plot(cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - XGBoost')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Large Language Model (LLM) Fine-tuning**\n\n** Load Pretrained Model and TokenizerWe use bert-base-uncased and adapt it for multi-class classification based on the number of developer roles. GPU is utilized if available.**\n","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nnum_labels = len(y.unique())\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(f\"Model: {model.config.model_type}, Labels: {num_labels}, Device: {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CommitMessageDataset(Dataset):\n    def __init__(self, commit_messages, labels, tokenizer, max_len):\n        self.commit_messages = commit_messages\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.commit_messages)\n\n    def __getitem__(self, item):\n        commit_message = str(self.commit_messages[item])\n        label = self.labels[item]\n        encoding = self.tokenizer.encode_plus(\n            commit_message,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        return {\n            'commit_message_text': commit_message,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nlabel_map = {class_name: i for i, class_name in enumerate(label_encoder.classes_)}\nMAX_LEN = 128\n\ntrain_dataset = CommitMessageDataset(msg_train.values, y_train.map(label_map).values, tokenizer, MAX_LEN)\nval_dataset = CommitMessageDataset(msg_val.values, y_val.map(label_map).values, tokenizer, MAX_LEN)\ntest_dataset = CommitMessageDataset(msg_test.values, y_test.map(label_map).values, tokenizer, MAX_LEN)\n\nprint(\"LLM datasets prepared:\", len(train_dataset), len(val_dataset), len(test_dataset))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 4\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nloss_fn = torch.nn.CrossEntropyLoss()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_llm_metrics = {\n    'macro_f1': history['val_macro_f1'][-1],\n    'accuracy': history['val_acc'][-1],\n    'precision': precision_score(history['val_y_true'][-1], history['val_y_pred'][-1], average='macro'),\n    'recall': recall_score(history['val_y_true'][-1], history['val_y_pred'][-1], average='macro')\n}\n\nall_models_metrics = {\n    'Logistic Regression': baseline_metrics,\n    'Random Forest': rf_metrics,\n    'XGBoost': xgb_metrics,\n    'Fine-tuned LLM': final_llm_metrics\n}\n\ncomparison_df_all = pd.DataFrame(all_models_metrics).T\ndisplay(comparison_df_all.sort_values(by='macro_f1', ascending=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Final Evaluation on Test Set (LLM)**\n\nMarkdown:\nThe fine-tuned LLM is evaluated on the unseen test set to report final metrics and the confusion matrix.","metadata":{}},{"cell_type":"code","source":"test_loss_llm, test_acc_llm, test_y_true_llm, test_y_pred_llm = eval_model(model, test_dataloader, loss_fn, device)\n\ntest_macro_f1_llm = f1_score(test_y_true_llm, test_y_pred_llm, average='macro')\ntest_accuracy_llm = accuracy_score(test_y_true_llm, test_y_pred_llm)\ntest_precision_llm = precision_score(test_y_true_llm, test_y_pred_llm, average='macro')\ntest_recall_llm = recall_score(test_y_true_llm, test_y_pred_llm, average='macro')\n\nprint(f'Test loss: {test_loss_llm:.4f}, Accuracy: {test_accuracy_llm:.4f}, Macro F1: {test_macro_f1_llm:.4f}')\nprint(classification_report(test_y_true_llm, test_y_pred_llm, target_names=label_encoder.classes_, digits=3))\n\ncm_llm_test = confusion_matrix(test_y_true_llm, test_y_pred_llm)\nConfusionMatrixDisplay(cm_llm_test, display_labels=label_encoder.classes_).plot(cmap='Blues', values_format='d')\nplt.title('Confusion Matrix - Fine-tuned LLM (Test Set)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}