{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12947081,"sourceType":"datasetVersion","datasetId":8193277}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# preprocessing.ipynb\n\n # Data Preprocessing\n\n#: This notebook covers the data loading, validation, and feature engineering steps for the developer role classification task.\n\n# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport ast\nimport nltk\nfrom datetime import datetime","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Loading and Validation**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Scikit-learn imports for potential preprocessing steps (though mostly handled in functions)\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\n\n\n# Define a function to load and validate the dataset.\ndef load_and_validate_data(file_path):\n    \"\"\"Loads the dataset from a CSV file and performs basic validation checks.\"\"\"\n    df = pd.read_csv(/content/final_dataset.csv)\n\n    # Validation checks\n    assert 'role' in df.columns, \"Target column 'role' missing\"\n    assert 'commitmessage' in df.columns, \"Commit message column missing\"\n\n    print(f\"Dataset shape: {df.shape}\")\n    print(\"\\nColumn types:\")\n    print(df.dtypes)\n\n    print(\"\\nMissing values:\")\n    print(df.isnull().sum())\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Preprocessing Functions**","metadata":{}},{"cell_type":"code","source":"# Define functions for file extension processing and time feature extraction.\ndef preprocess_file_extensions(ext_str):\n    \"\"\"Convert string representation of list to actual list\"\"\"\n    try:\n        return ast.literal_eval(ext_str)\n    except (ValueError, SyntaxError):\n        return []\n\ndef categorize_extensions(ext_list):\n    \"\"\"Categorize file extensions into frontend/backend/etc.\"\"\"\n    frontend_ext = ['js', 'css', 'html', 'vue', 'ts', 'jsx', 'tsx']\n    backend_ext = ['py', 'java', 'php', 'rb', 'go', 'c', 'cpp', 'h']\n    config_ext = ['json', 'yml', 'yaml', 'xml', 'config', 'properties']\n    test_ext = ['test', 'spec', 'test.js', 'test.py']\n\n    categories = {\n        'frontend': 0,\n        'backend': 0,\n        'config': 0,\n        'test': 0,\n        'other': 0\n    }\n\n    for ext in ext_list:\n        ext = ext.lower().replace('.', '')\n        if ext in frontend_ext:\n            categories['frontend'] += 1\n        elif ext in backend_ext:\n            categories['backend'] += 1\n        elif ext in config_ext:\n            categories['config'] += 1\n        elif ext in test_ext or 'test' in ext:\n            categories['test'] += 1\n        else:\n            categories['other'] += 1\n\n    return categories\n\ndef extract_time_features(datetime_str):\n    \"\"\"Extract temporal features from datetime string\"\"\"\n    dt = pd.to_datetime(datetime_str, errors='coerce')\n    if pd.isna(dt):\n        return {}\n\n    hour = dt.hour\n    weekday = dt.weekday()\n\n    # Cyclical encoding\n    hour_sin = np.sin(2 * np.pi * hour/24)\n    hour_cos = np.cos(2 * np.pi * hour/24)\n    day_sin = np.sin(2 * np.pi * weekday/7)\n    day_cos = np.cos(2 * np.pi * weekday/7)\n\n    return {\n        'hour_sin': hour_sin,\n        'hour_cos': hour_cos,\n        'day_sin': day_sin,\n        'day_cos': day_cos,\n        'is_weekend': 1 if weekday >= 5 else 0\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Main Preprocessing Pipeline** ","metadata":{}},{"cell_type":"code","source":"# Define the main function to apply all preprocessing steps.\ndef preprocess_data(df):\n    \"\"\"Apply all preprocessing steps\"\"\"\n    # Create a copy to avoid modifying the original\n    processed_df = df.copy()\n\n    # Process file extensions\n    processed_df['fileextensions'] = processed_df['fileextensions'].apply(preprocess_file_extensions)\n    ext_categories = processed_df['fileextensions'].apply(categorize_extensions)\n    ext_categories_df = pd.json_normalize(ext_categories)\n    processed_df = pd.concat([processed_df, ext_categories_df], axis=1)\n\n    # Process time features\n    time_features = processed_df['timeofcommit'].apply(extract_time_features)\n    time_features_df = pd.json_normalize(time_features)\n    processed_df = pd.concat([processed_df, time_features_df], axis=1)\n\n    # Add simple text features\n    processed_df['message_length'] = processed_df['commitmessage'].str.len()\n    processed_df['word_count'] = processed_df['commitmessage'].str.split().str.len()\n\n    # Add keyword features\n    keywords = ['bug', 'fix', 'feature', 'refactor', 'test', 'doc', 'merge']\n    for keyword in keywords:\n        processed_df[f'has_{keyword}'] = processed_df['commitmessage'].str.contains(\n            keyword, case=False).astype(int)\n\n    return processed_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Execute Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Load the data and apply the preprocessing pipeline.\ndf = load_and_validate_data('/content/final_dataset.csv')\nprocessed_df = preprocess_data(df)\n\n# Display the new features\nprint(\"New features after preprocessing:\")\nprint(processed_df.columns.tolist())\n\n# Display the first few rows of the processed DataFrame\nprint(\"\\nProcessed DataFrame head:\")\ndisplay(processed_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}